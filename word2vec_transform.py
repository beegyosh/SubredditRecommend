##
# word2vec_transform.py
##
#
# This script uses a pretrained word2vec model generated by
# word2vec_train.py in order to project comments into a
# vector space so that their clustering can be analyzed.
#
##

from pyspark import SparkContext
from pyspark import RDD
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql import HiveContext
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.clustering import *
from pyspark.ml.feature import *
from pyspark.ml.linalg import *
from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT
from pyspark.mllib.stat import Statistics
from fnmatch import fnmatch
import pickle
import string
import boto3
import botocore
import yaml

sc = SparkContext(appName = "Word2Vec Transform")
    
sqlContext = SQLContext(sc)

hiveContext = HiveContext(sc)
hiveContext.setConf("spark.sql.orc.filterPushdown", "true")

#load settings.yaml
with open("settings.yaml", 'r') as stream:
    try:
        settings = yaml.load(stream)
    except yaml.YAMLError as exc:
        print(exc)

comments = hiveContext.read.format("orc").load(settings['orc-data'])

#select author & body columns
comments = comments.select(comments['author'], comments['subreddit'], comments['body'])
#filter out authors who've deleted their accounts
comments = comments.filter(comments.author != "[deleted]")
#filter out deleted comments, can't be analyzed
comments = comments.filter(comments.body != "[deleted]")

#Tokenize comments for processing
tokenizer = RegexTokenizer(inputCol="body", outputCol="words") \
            .setPattern("[\\W_]+") \
            .setMinTokenLength(4)
comments = tokenizer.transform(comments)

#remove stop words
stopWordFile = open(settings['stop-word-file'])
sWords = stopWordFile.read().split('\n')
remover = StopWordsRemover(inputCol="words", outputCol="filtered", stopWords=sWords)
comments = remover.transform(comments)

model = Word2VecModel.load(settings['word2vec-model'])

subreddit_vectors = model.transform(comments)
author_vectors = subreddit_vectors

subreddit_vectors = subreddit_vectors.select('subreddit', 'result')
author_vectors = author_vectors.select('author', 'result')

#combine vectors for subreddits
subreddit_vectors = subreddit_vectors.rdd.mapValues(lambda v: v.toArray()) \
    .reduceByKey(lambda x, y: x + y) \
    .mapValues(lambda x: DenseVector(x)) \
    .toDF(['subreddit', 'vector'])
    
#combine vectors for authors
author_vectors = author_vectors.rdd.mapValues(lambda v: v.toArray()) \
    .reduceByKey(lambda x, y: x + y) \
    .mapValues(lambda x: DenseVector(x)) \
    .toDF(['author', 'vector'])
    
#save vector key-value pairs to S3
subreddit_vectors.write.mode('overwrite').save(settings['subreddit-vectors'])
author_vectors.write.mode('overwrite').save(settings['author-vectors'])







